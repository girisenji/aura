spring:
  application:
    name: aura-gateway
  
  # Enable Virtual Threads (Java 25)
  threads:
    virtual:
      enabled: true
  
  # Caching (using Caffeine by default, Redis optional)
  cache:
    type: caffeine
    caffeine:
      spec: maximumSize=10000,expireAfterWrite=1h
    cache-names:
      - llm-responses
      - rate-limits
  
  # Redis configuration (optional - uncomment for distributed deployments)
  # data:
  #   redis:
  #     host: ${REDIS_HOST:localhost}
  #     port: ${REDIS_PORT:6379}
  #     password: ${REDIS_PASSWORD:}
  #     timeout: 2000ms
  
  # Kafka configuration (for cost tracking)
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: 1
    consumer:
      group-id: aura-gateway
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer

# Server configuration
server:
  port: ${PORT:8080}
  compression:
    enabled: true
    mime-types: application/json,text/plain
  shutdown: graceful

# Management and Monitoring
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  health:
    redis:
      enabled: false  # Disable Redis health check since it's optional
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: ${spring.application.name}

# LLM Provider Configuration
aura:
  providers:
    openai:
      api-key: ${OPENAI_API_KEY:}
      base-url: https://api.openai.com/v1
      timeout: 60s
      max-retries: 2
      models:
        premium: gpt-4o
        balanced: gpt-4o-mini
        eco: gpt-3.5-turbo
    
    anthropic:
      api-key: ${ANTHROPIC_API_KEY:}
      base-url: https://api.anthropic.com
      timeout: 60s
      max-retries: 2
      models:
        premium: claude-3-5-sonnet-20241022
        balanced: claude-3-sonnet-20240229
        eco: claude-3-haiku-20240307
    
    azure:
      api-key: ${AZURE_OPENAI_API_KEY:}
      endpoint: ${AZURE_OPENAI_ENDPOINT:}
      deployment-name: ${AZURE_OPENAI_DEPLOYMENT:}
      api-version: 2024-02-15-preview
      timeout: 60s
    
    ollama:
      base-url: ${OLLAMA_BASE_URL:http://localhost:11434}
      timeout: 120s
      models:
        default: llama3
  
  # Classifier configuration
  classifier:
    model-path: models/all-MiniLM-L6-v2.onnx
    threshold:
      eco: 0.3
      balanced: 0.6
      premium: 0.8
  
  # Rate limiting
  rate-limit:
    enabled: true
    default-limit: 100
    window: 60s
  
  # Cost tracking
  cost-tracking:
    enabled: true
    kafka-topic: llm-usage-events
  
  # Guardrails
  guardrails:
    pii-masking:
      enabled: true
    content-moderation:
      enabled: false

# Logging
logging:
  level:
    root: INFO
    com.aura: DEBUG
    org.springframework.web: INFO
    dev.langchain4j: DEBUG
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"
